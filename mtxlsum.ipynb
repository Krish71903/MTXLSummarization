{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8043e23e",
   "metadata": {},
   "source": [
    "# Reproducing EMNLP 2024 Paper: Multi-Target Cross-Lingual Summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a272e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Install dependencies\n",
    "!pip install -q transformers datasets evaluate rouge-score sentence-transformers\n",
    "!pip install sonar-space\n",
    "!pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.6.0/cu124\n",
    "!pip install comet_ml\n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d703ce4",
   "metadata": {},
   "source": [
    "## 🔍 Load SONAR Encoder and Compute Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize SONAR embedding model\n",
    "sonar_model = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "def similarity_score(text1, text2):\n",
    "    emb = sonar_model.predict([text1, text2], source_lang=\"eng_Latn\")\n",
    "    return float(cosine_similarity([emb[0].cpu().numpy()], [emb[1].cpu().numpy()])[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b2b36",
   "metadata": {},
   "source": [
    "## ✨ Example: Re-Rank Candidate Summaries Based on Semantic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Example candidate summaries for demonstration\n",
    "candidates = {\n",
    "    'es': ['Bitcoin consume más energía que Argentina.', 'Bitcoin es muy costoso en electricidad.'],\n",
    "    'fr': ['Le Bitcoin consomme plus que l’Argentine.', 'Bitcoin a un coût énergétique élevé.']\n",
    "}\n",
    "\n",
    "# Re-rank using similarity\n",
    "from itertools import product\n",
    "\n",
    "def neutral_rr(candidates):\n",
    "    best_set = None\n",
    "    best_score = -1\n",
    "    for combo in product(*candidates.values()):\n",
    "        score = 0\n",
    "        for i in range(len(combo)):\n",
    "            for j in range(i+1, len(combo)):\n",
    "                score += similarity_score(combo[i], combo[j])\n",
    "        avg_score = score / (len(combo)*(len(combo)-1)/2)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_set = combo\n",
    "    return best_set, best_score\n",
    "\n",
    "best_summaries, score = neutral_rr(candidates)\n",
    "print('Best Semantically Coherent Summaries:')\n",
    "for lang, summ in zip(candidates.keys(), best_summaries):\n",
    "    print(f'{lang}: {summ}')\n",
    "print(f'Average Pairwise Similarity: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f459ff",
   "metadata": {},
   "source": [
    "## 🧠 Generate Summaries with mT5 (English → Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634bbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load mT5 fine-tuned on CrossSum\n",
    "model_name = \"csebuetnlp/mT5_m2m_crossSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
    "\n",
    "get_lang_id = lambda lang: tokenizer._convert_token_to_id(\n",
    "    model.config.task_specific_params[\"langid_map\"][lang][1]\n",
    ")\n",
    "\n",
    "def generate_summary(text, target_lang):\n",
    "    input_ids = tokenizer(\n",
    "        [WHITESPACE_HANDLER(text)],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        decoder_start_token_id=get_lang_id(target_lang),\n",
    "        max_length=168,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4,\n",
    "    )[0]\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        output_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b99924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example English news article\n",
    "text = \"Bitcoin uses more electricity annually than the whole of Argentina, analysis by Cambridge University suggests. Mining for the cryptocurrency is powerhungry, involving heavy computer calculations to verify transactions. Cambridge researchers say it consumes around 121.36 terawatt-hours (TWh) a year - and is unlikely to fall unless the value of the currency slumps\"\n",
    "\n",
    "langs = {\n",
    "    \"fr\": \"french\",\n",
    "    \"es\": \"spanish\",\n",
    "    \"pt\": \"portuguese\",\n",
    "    \"pu\": \"punjabi\",\n",
    "    \"ko\": \"korean\"\n",
    "}\n",
    "\n",
    "summaries = {}\n",
    "for code, full_lang in langs.items():\n",
    "    summary = generate_summary(text, target_lang=full_lang)\n",
    "    summaries[full_lang] = summary\n",
    "    print(f\"{full_lang} summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "candidates = {lang: [sum_text] for lang, sum_text in summaries.items()}\n",
    "\n",
    "best_set, best_score = neutral_rr(candidates)\n",
    "\n",
    "print(f\"\\nAverage NeutralRR similarity across all pairs: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbf690",
   "metadata": {},
   "source": [
    "## 🚀 Load CrossSum Cluster and apply mT5 and SONAR Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fe1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unbabel-comet\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Function to compute BLEU score\n",
    "def compute_bleu(reference, prediction):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], prediction.split(), smoothing_function=smoothie)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "def compute_scores(configs,num_samples,source_lang):\n",
    "  data = {\n",
    "    lang: load_dataset(\"csebuetnlp/CrossSum\", cfg)[\"test\"].select(range(num_samples))\n",
    "    for lang, cfg in configs.items()\n",
    "  }\n",
    "\n",
    "  rouge = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n",
    "  bleu  = load(\"bleu\")\n",
    "  comet = load(\"comet\", config_name=\"wmt20-comet-da\", device=device)\n",
    "\n",
    "  scores = {lang: defaultdict(list) for lang in configs}\n",
    "\n",
    "  for lang, ds in data.items():\n",
    "      r2_scores, bleu_scores, bl_scores, comet_scores = [], [], [], []\n",
    "      print(f\"\\nEvaluating language: {lang}\")\n",
    "      for idx, sample in enumerate(ds, 1):\n",
    "          src, ref = sample[\"text\"], sample[\"summary\"]\n",
    "          gen = generate_summary(src, target_lang=lang)\n",
    "\n",
    "\n",
    "          _, bl = neutral_rr({\"reference\":[ref], \"generated\":[gen]})\n",
    "          bl_scores.append(bl*10)\n",
    "\n",
    "          r2 = rouge.score(ref, gen)[\"rouge2\"].fmeasure\n",
    "          r2_scores.append(r2*100)\n",
    "\n",
    "          bleu = compute_bleu(ref, gen)\n",
    "          bleu_scores.append(bleu)\n",
    "\n",
    "          c = comet.compute(predictions=[gen], references=[ref], sources=[src])[\"scores\"][0]\n",
    "          comet_scores.append(c*100)\n",
    "\n",
    "          print(f\"[{idx:02d}/{len(ds)}] R2={r2:.4f}, BLEU={bleu:.4f}, BLASER={bl:.4f}, COMET={c:.4f}\")\n",
    "\n",
    "    # store averages\n",
    "      scores[lang][\"rouge2\"]  = sum(r2_scores) / len(r2_scores)\n",
    "      scores[lang][\"bleu\"]    = sum(bleu_scores) / len(bleu_scores)\n",
    "      scores[lang][\"blaser\"]  = sum(bl_scores) / len(bl_scores)\n",
    "      scores[lang][\"comet\"]   = sum(comet_scores) / len(comet_scores)\n",
    "\n",
    "# ─── Print summary ───────────────────────────────────────────────────────────\n",
    "  print(f\"\\nAverage scores over all languages for {source_lang}:\")\n",
    "  for lang, m in scores.items():\n",
    "      print(f\" • {lang.capitalize():7s} → \"\n",
    "          f\"ROUGE-2 {m['rouge2']:.4f}, \"\n",
    "          f\"BLEU {m['bleu']:.4f}, \"\n",
    "          f\"BLASER {m['blaser']:.4f}, \"\n",
    "          f\"COMET {m['comet']:.4f}\")\n",
    "\n",
    "  return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualiseScores(scores) \n",
    "  # Prepare data for plotting\n",
    "  languages = list(scores.keys())\n",
    "  rouge2_vals = [scores[lang][\"rouge2\"] for lang in languages]\n",
    "  bleu_vals   = [scores[lang][\"bleu\"] for lang in languages]\n",
    "  blaser_vals = [scores[lang][\"blaser\"] for lang in languages]\n",
    "  comet_vals  = [scores[lang][\"comet\"] for lang in languages]\n",
    "\n",
    "  # Set bar width and position\n",
    "  bar_width = 0.2\n",
    "  x = range(len(languages))\n",
    "\n",
    "  # Plotting\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  plt.bar([i - 1.5 * bar_width for i in x], rouge2_vals, width=bar_width, label='ROUGE-2')\n",
    "  plt.bar([i - 0.5 * bar_width for i in x], bleu_vals,   width=bar_width, label='BLEU')\n",
    "  plt.bar([i + 0.5 * bar_width for i in x], blaser_vals, width=bar_width, label='BLASER')\n",
    "  plt.bar([i + 1.5 * bar_width for i in x], comet_vals,  width=bar_width, label='COMET')\n",
    "\n",
    "  plt.xlabel(\"Language\")\n",
    "  plt.ylabel(\"Average Score\")\n",
    "  plt.title(\"Average Evaluation Metrics per Language\")\n",
    "  plt.xticks(ticks=x, labels=[lang.capitalize() for lang in languages])\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_en = {\n",
    "    \"english\": \"english-english\",\n",
    "    \"spanish\": \"english-spanish\",\n",
    "    \"french\":  \"english-french\",\n",
    "    \"arabic\":  \"english-arabic\",\n",
    "    \"russian\": \"english-russian\",\n",
    "    \"chinese_simplified\": \"english-chinese_simplified\"\n",
    "}\n",
    "\n",
    "configs_zh = {\n",
    "    \"english\": \"chinese_simplified-english\",\n",
    "    \"spanish\": \"chinese_simplified-spanish\",\n",
    "    \"french\": \"chinese_simplified-french\",\n",
    "    \"arabic\":  \"chinese_simplified-arabic\",\n",
    "    \"russian\": \"chinese_simplified-russian\",\n",
    "    \"chinese_simplified\": \"chinese_simplified-chinese_simplified\"\n",
    "}\n",
    "\n",
    "configs_fr = {\n",
    "    \"english\": \"english-english\",\n",
    "    \"spanish\": \"english-spanish\",\n",
    "    \"french\":  \"english-french\",\n",
    "    \"arabic\":  \"english-arabic\",\n",
    "    \"russian\": \"english-russian\",\n",
    "    \"chinese_simplified\": \"spanish-chinese_simplified\"\n",
    "}\n",
    "\n",
    "configs_es = {\n",
    "    \"english\": \"spanish-english\",\n",
    "    \"spanish\": \"spanish-spanish\",\n",
    "    \"french\":  \"spanish-french\",\n",
    "    \"arabic\":  \"spanish-arabic\",\n",
    "    \"russian\": \"spanish-russian\",\n",
    "    \"chinese_simplified\": \"spanish-chinese_simplified\"\n",
    "}\n",
    "\n",
    "scores_en = compute_scores(configs_en,50,\"English\")\n",
    "visualiseScores(scores_en)\n",
    "\n",
    "scores_zh = compute_scores(configs_zh,30,\"Chinese (simplified)\")\n",
    "visualiseScores(scores_zh)\n",
    "\n",
    "scores_fr = compute_scores(configs_fr,50,\"French\")\n",
    "visualiseScores(scores_fr)\n",
    "\n",
    "scores_es = compute_scores(configs_es,50,\"Spanish\")\n",
    "visualiseScores(scores_es)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
